<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Highly Capable Large Multimodal Models for Mobile Devices">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Imp</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="images/favicon.png" type="image/png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.16.0/gradio.js"></script>
</head>

<style>
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }

  .responsive-iframe-container {
    position: relative;
    padding-bottom: 56.25%;
    /* 16:9 aspect ratio */
    height: 0;
  }

  .responsive-iframe-container iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
  }

  .section {
    background-color: #efeff081;
    padding: 20px;
    /* 示例内边距 */
  }
</style>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Imp: <span class="is-size-2">Highly Capable Large Multimodal
                Models<br> for Mobile Devices </span></h1>
            <!-- <h3 class="title is-3 publication-title">Visual Instruction Tuning?</h3> -->
            <!-- <h5 class="subtitle is-5 publication-awards">NeurIPS 2023 (Oral)</h5> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;">Zhenwei Shao</span>,
              </span>
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;"> Zhou Yu<sup>&#x2020</sup></spam>,
              </span>
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;"> Jun Yu</spam>,
              </span>
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;"> Xuecheng Ouyang</spam>,
              </span>
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;"> Lihao Zheng</spam>,
              </span>
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;"> ZhenBiao Gai</spam>,
              </span>
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;"> Mingyang Wang</spam>,
              </span>
              <span class="author-block">
                <spam style="color:#c594d6;font-weight:normal;"> Jiajun Ding</spam>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#c594d6; font-weight:normal">&#x25B6 </b>Vision-Language Group@Media Intelligence Lab (MILVLG), Hangzhou Dianzi University</b></span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>&#x2020</sup>Project lead</span>
            </div>


            <!-- <div class="column has-text-centered">
            <h3 class="title is-3 publication-title">Improved Baselines with Visual Instruction Fine-tuning</h3>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://hliu.cc/" style="color:#f68946;font-weight:normal;">Haotian Liu<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://chunyuan.li/" style="color:#008AD7;font-weight:normal;">Chunyuan Li<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://yuheng-li.github.io" style="color:#008AD7;font-weight:normal;">Yuheng Li</a>,
              </span>
              <span class="author-block">
                <a href="https://pages.cs.wisc.edu/~yongjaelee/" style="color:#f68946;font-weight:normal;">Yong Jae
                  Lee</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> University of
                Wisconsin-Madison</b></span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Microsoft Research</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2405.12107/abs/2405.12107" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/MILVLG/imp" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/collections/MILVLG/imp-v15-664c07c27a71afa504f69cec" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-download"></i>
                    </span>
                    <span>Models</span>
                  </a>
                </span>
                <span class="link-block">
                  <!-- <a href="https://xmbot.net/imp/" target="_blank" -->
                  <a href="https://xmbot.net/imp/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-desktop"></i>
                      <!-- <i class="fas fa-robot"></i> -->
                    </span>
                    <span>Demo</span>
                    <!-- <span>ImpChat</span> -->
                  </a>
                </span>
                <span class="link-block">
                  <!-- <a href="https://xmbot.net/imp/" target="_blank" -->
                  <a href="https://github.com/MILVLG/mlc-imp" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-mobile"></i>
                      <!-- <i class="fas fa-robot"></i> -->
                    </span>
                    <span>Mobile</span>
                    <!-- <span>ImpChat</span> -->
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span> -->




                <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                  </a>
              </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-centered">
          <p><em>"A very small man can cast a very large shadow."</em></p>
          <p><em>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          ——George R.R. Martin, A Clash of Kings</em></p>
        </h4>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h4 class="subtitle has-text-justified">
          <!-- 🔥<span style="color: #ff3860">[NEW!]</span> LLaVA-1.5 achieves SoTA on 11 benchmarks, with just simple modifications to the original LLaVA, utilizes all public data, completes training in ~1 day on a single 8-A100 node, and surpasses methods that use billion-scale data.
          <br><br> -->
          We present a family of highly capable yet lightweight LMMs named Imp with models of 2B, 3B, and 4B<sup>*</sup>, which is a gift from our systematic study for lightweight LMMs from the aspects of model architecture, training strategy, and training data. Imp models steadily outperform the existing lightweight LMMs of similar size on most benchmarks, and even surpass the state-of-the-art LMMs at the 13B scale. With low-bit quantization and resolution reduction techniques, our Imp models can be deployed on mobile devices with a high inference speed.
        </h4>
        <p>
           * The model series presented in the paper are named Imp-v1.5 on Huggingface to distinguish from our Imp-v1-3B model released earlier.
        </p>
      </div>
    </div>
  </section>


  <!-- <section class="section" style="background-color:#efeff081">
    <div class="container is-max-desktop">
      <div class="responsive-iframe-container">
        <iframe src="https://www.bilibili.com/" style="width: 100%; height: 100%;" frameborder="0" allowfullscreen>
          <p>Your browser does not support the iframe tag.。</p>
        </iframe>
      </div>
    </div>
  </section> -->




  <section class="hero teaser">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> A Roadmap from LLaVA-1.5 to Imp</h2>
      </div>
    </div>
    <!-- </div> -->
    <!--/ Results. -->
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <h2 class="title is-4"> <span style="font-size: 100%;">1. Optimized Model Architectures</span></h2>
            <p>

              <!-- <ul type="1">
              <li><b>Stage 1: Optimized Model Architectures</b>. <span style="font-size: 95%;"></span></li> -->
              <ul type="1">
                <li><b>Choice of the LLM. </b> We
                  find that the LMM's performance is highly dependent on its supporting LLM. Simply replacing the LLM with a more lightweight one leads to worse performance.
                  What's more, conditioning on the same model scale, <a href="https://huggingface.co/microsoft/phi-2">Phi-2</a> is superior to other choices, like MobileLLaMA, due to its meticulously organized training data.
                </li>
                <li><b>Choice of the visual encoder. </b> 
                  A stronger visual encoder would yield a LMM with better vision-language capabilities.
                  We verify that visual encoders trained with vision-language contrastive
                  learning can obtain better visual
                  representations than those pre-trained with ImageNet classification. We choose a strong visual encoder, <a href="https://huggingface.co/google/siglip-so400m-patch14-384">SigLIP-SO/400M</a>, as the visual encoder.
                </li>
              </ul>
              <h2 class="title is-4"> <span style="font-size: 100%;">2. Improved Training Strategies</span></h2>
               <span style="font-size: 95%;">We follow LLaVA-1.5's two-stage training procedure, while exploring training strategies for lightweight LMMs.
                  As the first stage only acts as an initialization, it is of less importance compared to the second
                  stage. Therefore, we maintain the first-stage training settings in LLaVA-1.5.
                  <ul type="1">
                    <li><b>Finetuning mechanism.</b> We observe that the model trained with full-parameter funetuning
                      is inferior to the models with LoRA finetuning.
                      For LoRA finetuning, increasing rank from 128 to 256 brings a 0.2 point average score improvement,
                      while further increasing it to 512 leads to a 0.1 point decrease. Therefore, we finetune the LMMs with LoRA of rank 256.
                    <li><b>Number of training epochs.</b> Increasing training epochs from 1 to 2 brings a 0.5 point
                      improvement in average score. Meanwhile, further increasing training epochs from 2 to 3
                      leads to a 0.4 point decrease. Because of these, we set the number of training epochs to 2.
                </span></li>
            </ul>
            </ul>
            </p>
          </div>
          <centering>
            <div class="columns is-centered has-text-centered">
              <div class="column is-six-fifths"
                style="display: flex; align-items: flex-start; justify-content: center;">
                <figure style="text-align: center ;">
                  <img id="teaser" width="100%" src="images/fig1a.png">
                  <figcaption>
                    LLaVA-1.5's Model architecture
                  </figcaption>
                </figure>
                <figure style="text-align: center ;">
                  <img id="teaser" width="100%" src="images/fig1b.png">
                  <figcaption>
                    LLaVA-1.5's Training recipe
                  </figcaption>
                </figure>
                <figure style="text-align: center;">
                  <img id="teaser" width="73%" src="images/fig2.png">
                  <figcaption>
                    A Roadmap from LLaVA-1.5 to Imp
                  </figcaption>
                </figure>
                <!-- <figure style="text-align: match-parent ;">
              <img id="teaser" width="100%" src="images/images_evaluation.png">  
              <figcaption>
                Evaluation
              </figcaption> -->
                </figure>

              </div>
            </div>
            <!-- <div style="text-align: center;">
          <img id="teaser" width="70%" src="images/llava_arch.png">     
        </div> -->


          </centering>
          <br>
          <div class="content has-text-justified">
            <h2 class="title is-4"> <span style="font-size: 100%;">3. Augmented Instrucion-following Data.</span></h2>
            <p>
              Based on LLaVA's original 665K dataset, we remove the 22K TextCaps dataset which uses the same set of training images as TextVQA to ensure more strict zero-shot evaluation. Meanwhile, we append 32K OCR & chart-oriented datasets and 330K GPT4V-annotated datasets, resulting in 1M mixed vision-language instruction-tuning data in total. </br>
              <img id="painting_icon" width="98%" src="images/tab2.png">
            </div>
            Training on this augmented instrucion-following dataset, we further improve the performance of the lightweight LMMs.
            </div>
            </p>
          </div>
        
        </div>
      </div>
      <br><br>
  </section>

  <section class="hero teaser">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> Performance</h2>
      </div>
    </div>

    <!-- </div> -->
    <!--/ Results. -->    
  <div class="container is-max-desktop">
  
  
    <!-- Grounedtext2img. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4"> <span style="font-size: 100%;"> Quantitative Comparisons with the State-of-the-art LMMs
          benchmarks.</span></h2>
        
        <div>
          <!-- <a href="https://plotly.com/~lichunyuan24/5/?share_key=d78QObaCAYCIy8PJpe3gd1" target="_blank" title="llava_gpt4_pie" style="display: block; text-align: center;">   -->
            <img id="painting_icon" width="98%" src="images/tab4.png"> </a>
  
        </div>
        We apply these design choices to different lightweight LLMs, namely, Qwen-1.5 (1.8B), Phi-2 (2.7B), and Phi-3 (3.8B), to obtain a family of lightweight LMMs Imp-2B/3B/4B. We compare Imp models with state-of-the-art models on 10 commonly-used benchmarks.
        Imp-3B and Imp-4B steadily outperform all the existing
        lightweight LMMs on most benchmarks and even achieve competitive performance with LLaVA-
        1.5-13B, reflecting the effectiveness of our elaborated model design. 
        <!-- Moreover, compared with
        MiniCPM-V-3B, which uses over 300M training data, Imp-3B is much more data-efficient. This
        suggests that for lightweight LMMs, the quality of training data is more important than the
        quantity. Finally, although Imp-2B exhibits consistently worse performance than the rest two
        Imp models, its performance on the Chinese MMBCN benchmark is prominent. As our training
        data is purely in English, this phenomenon implies that the bilingual understanding capability
        of the LLM (i.e., Qwen-1.5) is successfully inherited by the LMM. -->
      </div>
    </div>
  
    <!-- Grounedtext2img. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4"> <span style="font-size: 100%;"> Quantitative Comparisons on Mobile Devices</span></h2>
        
        <div>
          <!-- <a href="https://plotly.com/~lichunyuan24/1/?share_key=v4opE3TJpxqQ08RYsDD4iv" target="_blank" title="Plot 1" style="display: block; text-align: center;"> -->
            <img id="painting_icon" width="98%" src="images/tab5.png">
          </a>
          <script data-plotly="lichunyuan24:1" sharekey-plotly="v4opE3TJpxqQ08RYsDD4iv" src="https://plotly.com/embed.js" async></script>
      </div>
      We conduct experiments of running Imp-3B on different hardware platforms, especially mobile devices. We explore the techniques of model quantization and image resolution reduction. To summarize,
           Imp-3B@196 with 4-bit quantization achieves a good balance in model storage,
            latency, and capability. We regard it as our default model for the deployment.
                
      </div>
    </div>
    <br>
  </section>

  <section class="hero teaser">

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"> Examples on Visual Instruction Following</h2>
      </div>
    </div>

    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-4">Visual Reasoning on two examples from <a href="https://arxiv.org/abs/2303.08774">OpenAI
            GPT-4 Technical Report</a></h2>
      </div>
    </div> -->
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <centering>
            <div class="columns is-centered has-text-centered">
              <div class="columns is-centered has-text-centered">
                <div class="column is-six-fifths"
                  style="display: flex; align-items: flex-start; justify-content: center;">
                  <!-- <img id="teaser" width="35%" src="images/cmp_ironing.png"> -->
                  <figure style="text-align: center ;">
                    <img id="teaser1" width="97%" src="images/fig3.png">
                    <img id="teaser2" width="97%" src="images/fig4.png">
                    <img id="teaser3" width="97%" src="images/fig5.png">
                  </figure>
                </div>
              </div>
            </div>
          </centering>
        </div>
      </div>
    </div>
    <br><br>
  </section>

  <section class="hero teaser">

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">ImpChat: LMM Assistants on Multiple Platforms</h2>
      </div>  
    </div>
    
    <!-- <div class="columns is-centered"> -->
    <div class="container is-max-desktop content">
        <p>We have developed a software suite of conversational AI interfaces named ImpChat, which is dedicated to providing users with an easy-to-use LMM assistant. Following is the UI demonstrations of ImpChat on multiple platforms. </p>
    </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <centering>
              <div class="columns is-centered has-text-centered">
                <div class="columns is-centered has-text-centered">
                  <div class="column is-six-fifths"
                    style="display: flex; align-items: flex-start; justify-content: center;">
                    <!-- <img id="teaser" width="35%" src="images/cmp_ironing.png"> -->
                    <figure style="text-align: center ;">
                      <img id="teaser" width="77.3%" src="images/fig6a.png">
                      <img id="teaser" width="21.5%" src="images/fig6b.jpg">
                    </figure>
                  </div>
                </div>
              </div>
            </centering>
          </div>
        </div>
      </div> 
      <br><br>
  </section>


  <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-4">Optical character recognition (OCR)</a></h2>
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
        <img id="teaser" width="32%" src="images/ocr/llava_example_cvpr2023.png">
        <img id="teaser" width="32%" src="images/ocr/llava_example_cvinw_logo.png">
        <img id="teaser" width="32%" src="images/ocr/example_llava_exmaple.png">
      </div>
    </div> -->

  <section class="hero teaser" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@article{imp2024,
  title={Imp: Highly Capable Large Multimodal Models for Mobile Devices},
  author={Shao, Zhenwei and Yu, Zhou and Yu, Jun and Ouyang, Xuecheng and Lihao, Zheng and Zhenbiao, Gai and Mingyang, Wang and Jiajun, Ding},
  journal={arXiv preprint arXiv:2405.12107},
  year={2024}
}
  </code></pre>
    </div>
    <br>
  </section>

  <section class="hero teaser" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a href="https://llava-vl.github.io">llava-vl.github.io</a>, <a
          href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>. We thank the LLaVA team、Microsoft、Google for
        giving us access to their models, and open-source projects, including Phi2 and Siglip.
      </p>

      <p>
        <b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only.
        They are also restricted to uses that follow the license agreement of LLaVA, Siglip, Phi2 and GPT-4. The dataset
        is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used
        outside of research purposes.
      </p>

    </div>
  </section>

</body>

</html>